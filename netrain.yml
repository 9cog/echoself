# NetRain Deep Tree Echo LLM Configuration
# Advanced neural training system for recursive echo architectures

name: deep_tree_echo_llm
version: 1.0.0
description: "Deep Tree Echo LLM with recursive attention and hierarchical reasoning"

# Model Architecture
model:
  type: DeepTreeEchoTransformer
  architecture:
    # Core transformer parameters
    n_layers: 4  # Reduced for CPU training
    n_heads: 8   # Reduced for CPU training
    n_embd: 256  # Reduced for CPU training
    block_size: 512  # Reduced for CPU training
    vocab_size: 50257  # GPT-2 tokenizer vocabulary
    
    # Deep tree echo specific
    tree_depth: 3  # Recursive tree depth (reduced for CPU)
    echo_layers: [1, 2, 3, 4]  # Layers with echo connections
    branch_factor: 2  # Number of branches per tree node (reduced for CPU)
    
    # Attention mechanisms
    attention:
      type: hierarchical_adaptive
      use_flash_attention: true
      attention_dropout: 0.1
      use_rotary_embeddings: true
      max_position_embeddings: 2048
      
    # Echo architecture
    echo:
      enable_recursive_attention: false
      echo_depth: 3
      echo_decay: 0.95
      use_memory_bank: false
      memory_size: 512
      
    # Tree structure
    tree:
      enable_hierarchical_pooling: true
      tree_attention_heads: 4
      branch_merge_strategy: "weighted_sum"
      use_gating_mechanism: true

# Data Configuration
data:
  dataset_name: "deep_echo_corpus"
  data_dir: "data/deep_echo"
  
  # Preprocessing
  preprocessing:
    tokenizer: "gpt2"
    max_length: 2048
    stride: 512
    add_special_tokens: true
    
  # Data augmentation
  augmentation:
    enable: true
    techniques:
      - token_masking: 0.15
      - span_masking: 0.1
      - sentence_permutation: 0.05
      - back_translation: false
      
  # Data loading
  loader:
    batch_size: 2
    num_workers: 1
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 2

# Training Configuration
training:
  # Optimization
  optimizer:
    type: "AdamW"
    learning_rate: 2e-4
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    
  # Learning rate schedule
  scheduler:
    type: "cosine_with_warmup"
    warmup_steps: 2000
    total_steps: 100000
    min_lr_ratio: 0.1
    
  # Training parameters
  max_steps: 100
  gradient_accumulation_steps: 2
  gradient_clipping: 1.0
  mixed_precision: "no"
  
  # Checkpointing
  checkpoint:
    save_steps: 1000
    save_total_limit: 5
    save_best_only: false
    monitor_metric: "eval_loss"
    
  # Evaluation
  evaluation:
    eval_steps: 500
    eval_strategy: "steps"
    metric_for_best_model: "eval_loss"
    greater_is_better: false
    
  # Early stopping
  early_stopping:
    enable: true
    patience: 10
    min_delta: 0.001

# Deep Tree Echo Specific Training
echo_training:
  # Progressive tree depth training
  progressive_depth:
    enable: true
    initial_depth: 1
    increment_every: 10000
    max_depth: 5
    
  # Echo loss components
  loss_weights:
    primary_loss: 1.0
    echo_consistency_loss: 0.3
    tree_structure_loss: 0.2
    hierarchical_loss: 0.1
    
  # Recursive attention training
  recursive_attention:
    enable_curriculum: true
    start_recursion_at: 5000
    max_recursion_depth: 3
    recursion_dropout: 0.1
    
  # Memory bank training
  memory_bank:
    update_frequency: 100
    memory_dropout: 0.1
    use_contrastive_loss: true
    temperature: 0.07

# Inference Configuration
inference:
  # Generation parameters
  generation:
    max_length: 512
    temperature: 0.8
    top_p: 0.9
    top_k: 50
    repetition_penalty: 1.1
    
  # Echo inference
  echo_inference:
    use_tree_beam_search: true
    beam_width: 4
    tree_branching_factor: 2
    echo_weight: 0.3
    
  # Optimization
  optimization:
    use_kv_cache: true
    use_flash_attention: true
    batch_size: 1

# Hardware Configuration
hardware:
  device: "cpu"
  device_ids: []
  distributed:
    enable: false
    backend: "nccl"
    
  # Memory optimization
  memory:
    gradient_checkpointing: true
    cpu_offload: false
    optimizer_offload: false
    
# Logging and Monitoring
logging:
  level: "INFO"
  log_dir: "logs/deep_tree_echo"
  
  # Wandb integration
  wandb:
    enable: true
    project: "deep-tree-echo-llm"
    entity: "netrain"
    tags: ["deep-tree", "echo", "hierarchical"]
    
  # TensorBoard
  tensorboard:
    enable: true
    log_dir: "runs/deep_tree_echo"
    
  # Metrics to track
  metrics:
    - loss
    - perplexity
    - echo_consistency
    - tree_depth_utilization
    - memory_bank_usage
    - learning_rate
    - gradient_norm

# Experiment Configuration
experiment:
  seed: 42
  deterministic: true
  name: "deep_tree_echo_v1"
  description: "Deep Tree Echo LLM with hierarchical recursive attention"
  
  # Hyperparameter search
  hyperparam_search:
    enable: false
    method: "bayesian"
    n_trials: 20
    
  # A/B testing
  ablation:
    enable: false
    components:
      - echo_layers
      - tree_structure
      - memory_bank